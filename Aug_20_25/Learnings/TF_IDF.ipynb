{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4eb0ee8",
   "metadata": {},
   "source": [
    "## 2. TF-IDF (Term Frequency – Inverse Document Frequency)\n",
    "- Adjusts word counts by importance:\n",
    "  - **TF**: Frequency of word in document.\n",
    "  - **IDF**: Penalizes words that appear in many documents.\n",
    "\n",
    "\n",
    "###  How it works\n",
    "TF-IDF is an improved version of Count Vectorizer.  \n",
    "Instead of raw counts, it assigns **weights** to words based on their **importance**.\n",
    "\n",
    "\n",
    "###  Formulae\n",
    "\n",
    "1. **Term Frequency (TF)**  \n",
    "   Measures how often a word appears in a document:  \n",
    "   $$\n",
    "   TF(t,d) = \\frac{f_{t,d}}{\\sum_{k} f_{k,d}}\n",
    "   $$  \n",
    "   where \\( f_{t,d} \\) = count of term *t* in document *d*,  \n",
    "   and denominator = total words in that document.  \n",
    "\n",
    "\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**  \n",
    "   Measures how important a word is across the corpus:  \n",
    "   $$\n",
    "   IDF(t) = \\log \\frac{N}{df_t}\n",
    "   $$  \n",
    "   where \\( N \\) = total number of documents,  \n",
    "   \\( df_t \\) = number of documents containing term *t*.  \n",
    "\n",
    "   Common words like \"the\" have low IDF, rare words get high IDF.  \n",
    "\n",
    "\n",
    "3. **TF-IDF**  \n",
    "   Final weight of a word in a document:  \n",
    "   $$\n",
    "   TFIDF(t,d) = TF(t,d) \\times IDF(t)\n",
    "   $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4329029a",
   "metadata": {},
   "source": [
    "\n",
    "Vocabulary = {\"I\", \"like\", \"NLP\", \"Machine\", \"Learning\", \"is\", \"fun\"}  \n",
    "\n",
    "**Step 1 – Term Frequency (TF):**\n",
    "\n",
    "| Document | I   | like | NLP | Machine | Learning | is  | fun |\n",
    "|----------|-----|------|-----|---------|----------|-----|-----|\n",
    "| Doc1     | 1/3 | 1/3  | 1/3 | 0       | 0        | 0   | 0   |\n",
    "| Doc2     | 1/4 | 1/4  | 0   | 1/4     | 1/4      | 0   | 0   |\n",
    "| Doc3     | 0   | 0    | 1/3 | 0       | 0        | 1/3 | 1/3 |\n",
    "\n",
    "**Step 2 – Inverse Document Frequency (IDF):**\n",
    "\n",
    "$$\n",
    "IDF = \\log \\frac{N}{df}\n",
    "$$\n",
    "\n",
    "- N = 3 (documents)  \n",
    "- Example: \"I\" appears in 2 docs → IDF(\"I\") = log(3/2)  \n",
    "\n",
    "So:  \n",
    "- IDF(I) = log(3/2)  \n",
    "- IDF(like) = log(3/2)  \n",
    "- IDF(NLP) = log(3/2)  \n",
    "- IDF(Machine) = log(3/1)  \n",
    "- IDF(Learning) = log(3/1)  \n",
    "- IDF(is) = log(3/1)  \n",
    "- IDF(fun) = log(3/1)  \n",
    "\n",
    "**Step 3 – TF × IDF = TF-IDF Matrix**  \n",
    "- Words unique to one document get **higher weight**.  \n",
    "- Common words like \"I\", \"like\" get **lower weight**.  \n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "- Down-weights common words (\"the\", \"is\").  \n",
    "- Emphasizes important, rare words.  \n",
    "- Works well for search engines and information retrieval.  \n",
    "\n",
    "---\n",
    "\n",
    "### Limitations\n",
    "1. Still **ignores context & word order**.  \n",
    "2. Similar words (\"car\" vs \"automobile\") are treated as different.  \n",
    "3. High dimensional for large vocabularies.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c790e1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
