{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae36bc1",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap Aggregating) in Machine Learning\n",
    "\n",
    "This notebook covers Bagging from scratch to end-to-end:\n",
    "- Introduction\n",
    "- Core Concepts\n",
    "- Algorithm\n",
    "- Mathematical Background\n",
    "- Intuition\n",
    "- Bagging with Decision Trees\n",
    "- Out-of-Bag Error\n",
    "- Advantages and Disadvantages\n",
    "- Implementation with Scikit-learn\n",
    "- Real-World Use Cases\n",
    "- Comparison with Other Ensemble Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061f718",
   "metadata": {},
   "source": [
    "## 1. What is Bagging?\n",
    "\n",
    "Bagging stands for **Bootstrap Aggregating**.  \n",
    "It is an ensemble learning method that combines predictions from multiple models to improve accuracy, stability, and robustness.\n",
    "\n",
    "- For classification → majority voting (most common class wins).  \n",
    "- For regression → average of predictions.  \n",
    "\n",
    "The main purpose of Bagging is to reduce **variance** (overfitting) without increasing **bias** too much.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db1826",
   "metadata": {},
   "source": [
    "## 2. Core Concepts\n",
    "\n",
    "### a) Ensemble Learning\n",
    "The idea that many weak learners together can form a strong learner.\n",
    "\n",
    "### b) Bootstrap Sampling\n",
    "- From a dataset of size N, create new datasets by sampling N points **with replacement**.\n",
    "- Each new dataset is called a *bootstrap sample*.\n",
    "- On average, about 63% of the original samples appear in each bootstrap sample, and the rest are *out-of-bag* samples.\n",
    "\n",
    "### c) Aggregation\n",
    "- Each bootstrap dataset trains a separate base model.\n",
    "- Predictions from models are combined:\n",
    "  - Classification: majority vote\n",
    "  - Regression: average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57612a6e",
   "metadata": {},
   "source": [
    "## 3. Bagging Algorithm (Step by Step)\n",
    "\n",
    "1. Start with training dataset \\( D = \\{(x_1,y_1), (x_2,y_2), …, (x_N,y_N)\\} \\).\n",
    "2. For \\( b = 1 \\) to \\( B \\) (number of models):\n",
    "   - Create bootstrap sample \\( D_b \\) by sampling \\( N \\) points with replacement.\n",
    "   - Train base model \\( f_b(x) \\) on \\( D_b \\).\n",
    "3. For a new test point \\( x \\):\n",
    "   - Get predictions from each model \\( f_b(x) \\).\n",
    "   - Combine results:\n",
    "     - Regression → \\( \\hat{y} = \\frac{1}{B} \\sum_{b=1}^B f_b(x) \\)\n",
    "     - Classification → \\( \\hat{y} = \\text{mode}\\{f_1(x), f_2(x), …, f_B(x)\\} \\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfbde9",
   "metadata": {},
   "source": [
    "## 4. Mathematical Understanding\n",
    "\n",
    "### Variance Reduction\n",
    "- A single model may have high variance.\n",
    "- Bagging reduces variance by averaging.\n",
    "\n",
    "If models are independent with variance \\( \\sigma^2 \\), the variance after averaging \\( B \\) models is:\n",
    "\n",
    "$$\n",
    "\\text{Var(Bagging)} = \\frac{\\sigma^2}{B}\n",
    "$$\n",
    "\n",
    "In practice, models are not fully independent, but variance is still reduced significantly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63035ddd",
   "metadata": {},
   "source": [
    "## 5. Intuition (Coin Toss Analogy)\n",
    "\n",
    "- Imagine one person guessing coin tosses: 50% accuracy.\n",
    "- If 100 people guess independently and we take the majority vote, the probability of being correct increases.\n",
    "- Bagging works the same way: many unstable models combined are more reliable than one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a1c3c",
   "metadata": {},
   "source": [
    "## 6. Bagging with Decision Trees\n",
    "\n",
    "- Bagging is usually applied with **Decision Trees**.\n",
    "- Decision Trees are high-variance models, so bagging stabilizes them.\n",
    "- An ensemble of bagged decision trees is also the foundation for **Random Forests**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96624b99",
   "metadata": {},
   "source": [
    "## 7. Out-of-Bag (OOB) Error\n",
    "\n",
    "- On average, ~36% of the samples are not included in a bootstrap sample.\n",
    "- These left-out samples are called **out-of-bag (OOB)** samples.\n",
    "- They can be used to estimate error without needing a separate validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5aaa55",
   "metadata": {},
   "source": [
    "## 8. Advantages of Bagging\n",
    "\n",
    "- Reduces variance and prevents overfitting\n",
    "- Works well with unstable learners (e.g., decision trees)\n",
    "- OOB error provides internal validation\n",
    "- Easy to parallelize\n",
    "\n",
    "## 9. Disadvantages of Bagging\n",
    "\n",
    "- Does not reduce bias\n",
    "- Requires training many models (higher computation)\n",
    "- Models may still be correlated, reducing variance reduction effectiveness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dad7b6",
   "metadata": {},
   "source": [
    "## 10. Real-World Use Cases\n",
    "\n",
    "- Finance: credit risk analysis\n",
    "- Medicine: disease prediction\n",
    "- Marketing: customer churn prediction\n",
    "- Machine Learning Competitions: improving accuracy with ensembles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021b21c",
   "metadata": {},
   "source": [
    "## 11. Bagging vs Other Ensemble Methods\n",
    "\n",
    "- **Bagging**: Trains models independently on bootstrapped samples; reduces variance.\n",
    "- **Boosting**: Trains models sequentially, focusing on mistakes; reduces bias.\n",
    "- **Stacking**: Combines predictions of multiple models using a meta-learner.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
