{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c983dc10",
   "metadata": {},
   "source": [
    "# Boosting in Machine Learning\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Boosting is an **ensemble learning method** that combines multiple *weak learners* (models that perform slightly better than random guessing) into a single strong learner.\n",
    "\n",
    "- Unlike Bagging (which trains learners independently on bootstrap samples), **Boosting trains models sequentially**.  \n",
    "- Each new model focuses on the **errors (misclassified data points)** made by the previous ones.  \n",
    "- Final prediction is a weighted majority vote (classification) or weighted sum (regression).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Key Idea\n",
    "\n",
    "1. Train a weak learner on the data.\n",
    "2. Evaluate errors.\n",
    "3. Increase the weights of misclassified samples so the next learner focuses more on them.\n",
    "4. Repeat this process for multiple rounds.\n",
    "5. Combine all weak learners into a final strong model.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Boosting Algorithm (General)\n",
    "\n",
    "Given training dataset $D = \\{(x_1, y_1), (x_2, y_2), \\dots, (x_N, y_N)\\}$:\n",
    "\n",
    "1. Initialize weights:\n",
    "   $$\n",
    "   w_i = \\frac{1}{N}, \\quad i = 1, 2, \\dots, N\n",
    "   $$\n",
    "   Each data point has equal weight initially.\n",
    "\n",
    "2. For $m = 1$ to $M$ (number of weak learners):\n",
    "   - Train weak learner $h_m(x)$ using weights $w_i$.\n",
    "   - Compute weighted error:\n",
    "     $$\n",
    "     \\epsilon_m = \\frac{\\sum_{i=1}^N w_i \\cdot I(y_i \\neq h_m(x_i))}{\\sum_{i=1}^N w_i}\n",
    "     $$\n",
    "   - Compute learner weight:\n",
    "     $$\n",
    "     \\alpha_m = \\frac{1}{2} \\ln \\left(\\frac{1 - \\epsilon_m}{\\epsilon_m}\\right)\n",
    "     $$\n",
    "   - Update sample weights:\n",
    "     $$\n",
    "     w_i \\leftarrow w_i \\cdot \\exp\\big(-\\alpha_m y_i h_m(x_i)\\big)\n",
    "     $$\n",
    "   - Normalize $w_i$ so that $\\sum w_i = 1$.\n",
    "\n",
    "3. Final model (strong learner):\n",
    "   $$\n",
    "   H(x) = \\text{sign}\\left(\\sum_{m=1}^M \\alpha_m h_m(x)\\right)\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Types of Boosting\n",
    "\n",
    "### 4.1 AdaBoost (Adaptive Boosting)\n",
    "- Original and most famous boosting algorithm.\n",
    "- Uses decision stumps (1-level decision trees) as weak learners.\n",
    "- Misclassified samples get higher weights at each step.\n",
    "\n",
    "### 4.2 Gradient Boosting\n",
    "- Instead of adjusting weights, it fits new learners to the **residual errors** of previous learners.\n",
    "- Uses gradient descent to minimize a loss function.\n",
    "\n",
    "Mathematical idea:\n",
    "$$\n",
    "F_{m}(x) = F_{m-1}(x) + \\eta h_m(x)\n",
    "$$\n",
    "where $\\eta$ is the learning rate and $h_m$ is the weak learner trained on residuals.\n",
    "\n",
    "### 4.3 XGBoost (Extreme Gradient Boosting)\n",
    "- Optimized version of Gradient Boosting.\n",
    "- Features: regularization, parallelization, tree pruning, handling missing values.\n",
    "- Very popular in Kaggle competitions.\n",
    "\n",
    "### 4.4 LightGBM & CatBoost\n",
    "- Variants of boosting optimized for **speed, large datasets, and categorical features**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Advantages & Disadvantages\n",
    "\n",
    "**Advantages**\n",
    "- High accuracy compared to single models.\n",
    "- Works well with weak learners (like decision stumps).\n",
    "- Handles bias effectively.\n",
    "\n",
    "**Disadvantages**\n",
    "- Sensitive to noisy data and outliers (because it gives higher weight to hard points).\n",
    "- Slower than bagging methods.\n",
    "- Can overfit if too many learners are used.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary\n",
    "\n",
    "- **Boosting** combines weak learners sequentially, with each focusing on previous mistakes.\n",
    "- **AdaBoost** → adjusts sample weights.\n",
    "- **Gradient Boosting** → fits residuals using gradient descent.\n",
    "- **XGBoost, LightGBM, CatBoost** → modern, optimized boosting frameworks.\n",
    "\n",
    "Boosting is one of the most powerful ML techniques and is widely used in real-world competitions and production systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7012c5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
