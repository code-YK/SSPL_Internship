{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa91888",
   "metadata": {},
   "source": [
    "## 1. Feature Scaling\n",
    "\n",
    "* Feature scaling in Machine Learning is the process of transforming numerical features so they are on a similar scale.\n",
    "* If feature scaling is not done then machine learning algorithm tends to use greater values as higher and consider smaller values as lower regardless of the unit of the values.\n",
    "* For example - 10cm and 10m will be same for machine learning algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f8663e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LotArea  MSSubClass\n",
      "0     8450          60\n",
      "1     9600          20\n",
      "2    11250          60\n",
      "3     9550          70\n",
      "4    14260          60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('SampleFile.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583130f4",
   "metadata": {},
   "source": [
    "### Min - Max Scaling \n",
    "This method of scaling requires below two-step:\n",
    "\n",
    "1. First we are supposed to find the minimum and the maximum value of the column.\n",
    "2. Then we will subtract the minimum value from the entry and divide the result by the difference between the maximum and the minimum value.\n",
    "\n",
    "Data is ranged between 0 to 1 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a67c7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>MSSubClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033420</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.038795</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046507</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038561</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.060576</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    LotArea  MSSubClass\n",
       "0  0.033420    0.235294\n",
       "1  0.038795    0.000000\n",
       "2  0.046507    0.235294\n",
       "3  0.038561    0.294118\n",
       "4  0.060576    0.235294"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data, \n",
    "                         columns=df.columns)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea35740",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "This method of scaling is basically based on the central tendencies and variance of the data. \n",
    "\n",
    "1. First we should calculate the mean and standard deviation of the data we would like to normalize it.\n",
    "2. Then we are supposed to subtract the mean value from each entry and then divide the result by the standard deviation.\n",
    "\n",
    "Centers data at mean 0 with standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b06df7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    LotArea  MSSubClass\n",
      "0 -0.207142    0.073375\n",
      "1 -0.091886   -0.872563\n",
      "2  0.073480    0.073375\n",
      "3 -0.096897    0.309859\n",
      "4  0.375148    0.073375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data,\n",
    "                         columns=df.columns)\n",
    "print(scaled_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc0c88c",
   "metadata": {},
   "source": [
    "### Robust Scaling\n",
    "In this method of scaling, we use two main statistical measures of the data.\n",
    "* Median\n",
    "* Inter-Quartile Range\n",
    "\n",
    "After calculating these two values we are supposed to subtract the median from each entry and then divide the result by the interquartile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b72eace6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    LotArea  MSSubClass\n",
      "0 -0.254076         0.2\n",
      "1  0.030015        -0.6\n",
      "2  0.437624         0.2\n",
      "3  0.017663         0.4\n",
      "4  1.181201         0.2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data,  \n",
    "                         columns=df.columns)\n",
    "print(scaled_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c832c2",
   "metadata": {},
   "source": [
    "## 2. Encoding Techniques\n",
    "- **Encoding** is the process of converting **categorical data** (text or labels) into **numeric format** so that machine learning algorithms can understand and process it.\n",
    "- Many ML algorithms work only with numerical inputs.\n",
    "\n",
    "Reasons : \n",
    "- Computers interpret numbers more efficiently than text.\n",
    "- ML algorithms like **Linear Regression**, **Logistic Regression**, **SVM**, and most tree-based methods require numeric data.\n",
    "- Helps in **feature engineering** and improves **model performance**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d0337f",
   "metadata": {},
   "source": [
    "### Types of Encoding\n",
    "\n",
    "#### 1. Label Encoding\n",
    "- Assigns a **unique integer** to each category.\n",
    "- Ex - ['red', 'green', 'Blue'] -> [0, 1, 2]\n",
    "\n",
    "- **Pros:** Simple, no extra columns created.  \n",
    "- **Cons:** Can mislead algorithms into thinking there’s an **ordinal relationship** between categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6809b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Data: [2 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = ['Red', 'Green', 'Blue', 'Red']\n",
    "\n",
    "le = LabelEncoder()\n",
    "encoded_data = le.fit_transform(data)\n",
    "print(f\"Encoded Data: {encoded_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66007d0",
   "metadata": {},
   "source": [
    "#### 2. One-Hot Encoding\n",
    "- Creates a binary column for each category.\n",
    "- ex:\n",
    "        Red   → [1, 0, 0]\n",
    "        Blue  → [0, 1, 0]\n",
    "        Green → [0, 0, 1]\n",
    "\n",
    "- Pros: No ordinal assumption.\n",
    "- Cons: Increases dimensionality for large categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7861735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Blue  Green    Red\n",
      "0  False  False   True\n",
      "1   True  False  False\n",
      "2  False   True  False\n",
      "3  False  False   True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = ['Red', 'Blue', 'Green', 'Red']\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Color'])\n",
    "one_hot_encoded = pd.get_dummies(df['Color'])\n",
    "\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37feb128",
   "metadata": {},
   "source": [
    "#### 3. Ordinal Encoding\n",
    "- Ordinal Encoding is used for ordinal data, where categories have a natural order.\n",
    "\n",
    "- Assigns integers to categories based on their order.\n",
    "\n",
    "- Ex - [small, medium, large] -> [1,2,3]\n",
    "\n",
    "- Pros: Maintains order; reduces dimensionality.\n",
    "\n",
    "- Cons: Not suitable for nominal categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4aace22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Ordinal Data:\n",
      " [[0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "data = [['Low'], ['Medium'], ['High'], ['Medium'], ['Low']]\n",
    "\n",
    "encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "\n",
    "print(f\"Encoded Ordinal Data:\\n {encoded_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3558193",
   "metadata": {},
   "source": [
    "#### 4. Target Encoding\n",
    "- Replaces a category with the mean of the target variable for that category.\n",
    "- This technique is especially useful when there is a relationship between the categorical feature and the target variable.\n",
    "- Pros: Captures relationship to target variable.\n",
    "- Cons: Risk of overfitting; must apply smoothing/statistical techniques and ensure leakage prevention (e.g., CV or holdout techniques).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8fe738b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from category_encoders) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from category_encoders) (2.2.2)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from category_encoders) (0.5.6)\n",
      "Requirement already satisfied: scikit-learn>=1.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from category_encoders) (1.7.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from category_encoders) (1.13.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from category_encoders) (0.14.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2023.3)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\anaconda3\\lib\\site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn>=1.6.0->category_encoders) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn>=1.6.0->category_encoders) (3.5.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from statsmodels>=0.9.0->category_encoders) (24.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9955e136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Target Data:\n",
      "       City\n",
      "0  0.570926\n",
      "1  0.434946\n",
      "2  0.570926\n",
      "3  0.434946\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'City': ['London', 'Paris', 'London', 'Berlin'], 'Target': [1, 0, 1, 0]}\n",
    ")\n",
    "\n",
    "encoder = ce.TargetEncoder(cols=['City'])\n",
    "df_tgt = encoder.fit_transform(df['City'], df['Target'])\n",
    "\n",
    "print(f\"Encoded Target Data:\\n{df_tgt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f84cd",
   "metadata": {},
   "source": [
    "#### 5.Frequency Encoding\n",
    "- Frequency Encoding assigns each category a value based on its frequency in the dataset.\n",
    "- This technique can be useful for handling features with many unique categories.\n",
    "- Pros: Low computational and storage requirements.\n",
    "- Cons: Can introduce data leakage if not handled properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba2b240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Data: [3, 1, 1, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = ['Red', 'Green', 'Blue', 'Red', 'Red']\n",
    "series_data = pd.Series(data)\n",
    "frequency_encoding = series_data.value_counts()\n",
    "\n",
    "encoded_data = [frequency_encoding[x] for x in data]\n",
    "print(\"Encoded Data:\", encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7889a404",
   "metadata": {},
   "source": [
    "## 3. Imputation Techniques\n",
    "- **Imputation** is the process of replacing **missing or null values** in a dataset with substituted values.\n",
    "- Many ML algorithms **cannot handle missing values directly**.\n",
    "- Proper imputation improves **data quality** and **model performance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade49198",
   "metadata": {},
   "source": [
    "##### **Types of Missing Data**\n",
    "1. **MCAR (Missing Completely at Random)**  \n",
    "   - Missing values have **no relationship** to other data.\n",
    "2. **MAR (Missing at Random)**  \n",
    "   - Missingness depends on **observed variables**.\n",
    "3. **MNAR (Missing Not at Random)**  \n",
    "   - Missingness depends on **unobserved variables**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5953e",
   "metadata": {},
   "source": [
    "#### **1. Mean / Median / Mode Imputation**\n",
    "- *Mean* → for numerical data (normally distributed)\n",
    "- *Median* → for numerical data (skewed distribution)\n",
    "- *Mode* → for categorical data\n",
    "\n",
    "- Pros: Simple and fast\n",
    "- Cons: Ignores relationships between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "350f9fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 2, np.nan, 4, 5])\n",
    "\n",
    "mean_value = np.nanmean(data)\n",
    "\n",
    "data = np.where(np.isnan(data), mean_value, data)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aea4ac",
   "metadata": {},
   "source": [
    "### 2. Constant Value Imputation\n",
    "- Replace missing values with a specific constant (e.g., 0, \"Unknown\").\n",
    "- Pros: Useful for special meaning values\n",
    "- Cons: May create artificial patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37617fdf",
   "metadata": {},
   "source": [
    "### 3. Forward Fill / Backward Fill\n",
    "- Forward Fill: Fill with previous value\n",
    "- Backward Fill: Fill with next value\n",
    "- Pros: Works well in time-series data\n",
    "- Cons: May propagate incorrect values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc37a10",
   "metadata": {},
   "source": [
    "### 4. KNN Imputation\n",
    "- This technique imputes missing values using the 𝑘 nearest neighbors based on other variables. \n",
    "- It’s useful when there is a strong correlation between features.\n",
    "- Pros: Captures feature relationships\n",
    "- Cons: Computationally expensive for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37b86c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset After KNN Imputation:\n",
      " [[1. 2. 6.]\n",
      " [3. 4. 5.]\n",
      " [4. 6. 7.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "data = np.array([[1, 2, np.nan], \n",
    "                 [3, np.nan, 5], \n",
    "                 [4, 6, 7]])\n",
    "\n",
    "# Initialize the KNN Imputer with 2 neighbors\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "\n",
    "# Apply KNN Imputation\n",
    "data_imputed = imputer.fit_transform(data)\n",
    "\n",
    "print(\"Dataset After KNN Imputation:\\n\", data_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f110293b",
   "metadata": {},
   "source": [
    "### 5. Multivariate Imputation by Chained Equations (MICE)\n",
    "- Models each feature with missing values as a function of other features.\n",
    "- Multiple imputation generates several imputed datasets by creating multiple plausible values for the missing data and combining the results.\n",
    "- Steps:\n",
    "    1. Generate 𝑚 imputed datasets.\n",
    "    2. Analyze each dataset.\n",
    "    3. Pool the results using: Estimation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5e2cc",
   "metadata": {},
   "source": [
    "### 6. Regression Imputation\n",
    "- Predicts missing values using a regression model trained on other features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147f950",
   "metadata": {},
   "source": [
    "### 7. Dropping Missing Values\n",
    "- Drop rows or drop columns with too many missing values.\n",
    "- If certain feature have more than 80% Null then drop the feature\n",
    "- Pros: Removes uncertainty completely\n",
    "- Cons: Can cause data loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ed2b49",
   "metadata": {},
   "source": [
    "##### Best Practices\n",
    "\n",
    "- Identify missing data type (MCAR, MAR, MNAR) before choosing method.\n",
    "\n",
    "- Avoid data leakage (impute after train-test split).\n",
    "\n",
    "- For time-series, prefer forward/backward fill.\n",
    "\n",
    "- For complex datasets, consider KNN or MICE.\n",
    "\n",
    "- Compare model performance after imputation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f19ceb",
   "metadata": {},
   "source": [
    "## 4. Feature Selection \n",
    "- Feature selection is the process of choosing the most important input variables (features) from your dataset that contribute the most to predicting the target variable.\n",
    "- Instead of feeding all available features to a model, we pick only the relevant ones to:\n",
    "\n",
    "    - Reduce overfitting\n",
    "    - Improve model accuracy\n",
    "    - Make models faster and easier to interpret\n",
    "    - Reduce training cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4483a6b",
   "metadata": {},
   "source": [
    "- ##### Feature Selection Techniques\n",
    "| **Category**         | **How it Works**                                                                                         | **Example Techniques**                                                       |\n",
    "| -------------------- | -------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |\n",
    "| **Filter Methods**   | Select features based on statistical scores before model training. Independent of ML algorithm.          | Correlation Coefficient, Chi-square test, ANOVA F-test, Mutual Information   |\n",
    "| **Wrapper Methods**  | Evaluate subsets of features by training a model multiple times and choosing the best performing subset. | Forward Selection, Backward Elimination, Recursive Feature Elimination (RFE) |\n",
    "| **Embedded Methods** | Feature selection happens automatically during model training.                                           | Lasso (L1) Regularization, Decision Tree Feature Importance, ElasticNet      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420321f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ed3e995",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
