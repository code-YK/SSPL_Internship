{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea4ebc31",
   "metadata": {},
   "source": [
    "# üìù Notes on Optimizers in Deep Learning  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction  \n",
    "Optimizers are algorithms used to **update the weights** of a neural network in order to minimize the **loss function**.  \n",
    "They determine **how the model learns** and how quickly it converges.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Key Terms You Must Know  \n",
    "\n",
    "### Loss Function (Cost Function)  \n",
    "- A measure of error between predictions and actual values.  \n",
    "- Example:  \n",
    "  - Mean Squared Error (MSE) ‚Üí Regression  \n",
    "  - Cross-Entropy Loss ‚Üí Classification  \n",
    "\n",
    "### Gradient  \n",
    "- The **partial derivative of the loss** with respect to weights.  \n",
    "- Shows the **direction** to update weights to reduce error.  \n",
    "\n",
    "### Learning Rate (Œ∑)  \n",
    "- A hyperparameter that controls the **step size** in updating weights.  \n",
    "- Small Œ∑ ‚Üí slow learning  \n",
    "- Large Œ∑ ‚Üí unstable, may diverge  \n",
    "\n",
    "### Weight Update Rule  \n",
    "General formula:  \n",
    "$$\n",
    "w = w - \\eta \\cdot \\nabla L\n",
    "$$  \n",
    "**Where:**\n",
    "- $w$: weight (parameter)\n",
    "- $\\eta$: learning rate\n",
    "- $\\nabla L$: gradient of the loss with respect to $w$\n",
    "\n",
    "### Epoch  \n",
    "- One complete pass through the **entire training dataset**.  \n",
    "\n",
    "### Batch & Mini-Batch  \n",
    "- **Batch Gradient Descent:** Uses the full dataset to compute gradient.  \n",
    "- **Stochastic Gradient Descent (SGD):** Updates weights after every single sample.  \n",
    "- **Mini-Batch Gradient Descent:** Uses small subsets (common in practice).  \n",
    "\n",
    "### Convergence  \n",
    "- The point where the model stops improving (loss stabilizes).  \n",
    "\n",
    "### Momentum  \n",
    "- Technique that **remembers past gradients** to smooth updates.  \n",
    "- Helps avoid zig-zagging and speeds up convergence.  \n",
    "\n",
    "### Adaptive Learning Rate  \n",
    "- Optimizers like **RMSProp** and **Adam** adjust the learning rate for each parameter automatically.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fd464b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Gradient Descent (Batch GD)  \n",
    "\n",
    "- Uses the **entire dataset** to compute the gradient before updating weights.  \n",
    "- Accurate but very **slow** for large datasets.  \n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\,\\nabla_w \\mathcal{L}(w)\n",
    "$$  \n",
    "\n",
    "**Key points:**  \n",
    "- Stable but computationally expensive.  \n",
    "- Rarely used in pure form; more practical variants exist.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Stochastic Gradient Descent (SGD)  \n",
    "\n",
    "- Updates weights **after every single training sample**.  \n",
    "- Much faster than batch gradient descent, but very noisy.  \n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\,\\nabla_w \\mathcal{L}(w; x_i, y_i)\n",
    "$$  \n",
    "\n",
    "Where $(x_i, y_i)$ is a single training sample.  \n",
    "\n",
    "**Key points:**  \n",
    "- Introduces randomness (helps escape local minima).  \n",
    "- Can oscillate heavily around minima.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Mini-Batch Gradient Descent  \n",
    "\n",
    "- Uses a **small subset (batch)** of training data for each update.  \n",
    "- Most widely used in practice.  \n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\,\\nabla_w \\mathcal{L}(w; B)\n",
    "$$  \n",
    "\n",
    "Where $B$ = batch of samples.  \n",
    "\n",
    "**Key points:**  \n",
    "- Efficient and balances stability vs. speed.  \n",
    "- Supported by GPUs easily.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. SGD with Momentum  \n",
    "\n",
    "- Adds a **momentum term** that remembers past gradients to smooth updates.  \n",
    "- Helps accelerate learning in relevant directions and dampens oscillations.  \n",
    "\n",
    "Update rules:  \n",
    "\n",
    "$$\n",
    "v_t = \\gamma v_{t-1} + \\eta \\,\\nabla_w \\mathcal{L}(w_t)\n",
    "$$  \n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - v_t\n",
    "$$  \n",
    "\n",
    "Where:  \n",
    "- $v_t$: velocity (accumulated gradient)  \n",
    "- $\\gamma$: momentum coefficient (typically 0.9)  \n",
    "\n",
    "**Key points:**  \n",
    "- Faster convergence than vanilla SGD.  \n",
    "- Reduces zig-zagging, especially in ravines.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. RMSProp (Root Mean Square Propagation)  \n",
    "\n",
    "- Adjusts the learning rate for each parameter individually.  \n",
    "- Uses an exponentially decaying average of squared gradients.  \n",
    "\n",
    "Update rules:  \n",
    "\n",
    "$$\n",
    "E[g^2]_t = \\beta E[g^2]_{t-1} + (1-\\beta) g_t^2\n",
    "$$  \n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\, g_t\n",
    "$$  \n",
    "\n",
    "Where:  \n",
    "- $g_t$: gradient at time $t$  \n",
    "- $\\beta$: decay rate (commonly 0.9)  \n",
    "- $\\epsilon$: small constant to avoid division by zero  \n",
    "\n",
    "**Key points:**  \n",
    "- Good for non-stationary problems and RNNs.  \n",
    "- Handles exploding/vanishing gradients better.  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Adam (Adaptive Moment Estimation)  \n",
    "\n",
    "- Combines **Momentum + RMSProp**.  \n",
    "- Maintains moving averages of both gradients and squared gradients.  \n",
    "\n",
    "Update rules:  \n",
    "\n",
    "1. Compute moving averages:  \n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n",
    "$$  \n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n",
    "$$  \n",
    "\n",
    "2. Bias correction:  \n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \n",
    "\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\n",
    "$$  \n",
    "\n",
    "3. Parameter update:  \n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\,\\hat{m}_t\n",
    "$$  \n",
    "\n",
    "Where:  \n",
    "- $\\beta_1 \\approx 0.9$, $\\beta_2 \\approx 0.999$  \n",
    "- $\\epsilon$ is a small constant (e.g., $10^{-8}$)  \n",
    "\n",
    "**Key points:**  \n",
    "- Default optimizer in many frameworks (fast, reliable).  \n",
    "- Works well in most problems, but sometimes generalization is weaker than SGD.  \n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table  \n",
    "\n",
    "| Optimizer | Formula (Core) | Key Feature | Pros | Cons |\n",
    "|-----------|----------------|-------------|------|------|\n",
    "| Gradient Descent | $w \\leftarrow w - \\eta \\nabla L$ | Full dataset | Stable | Very slow |\n",
    "| SGD | $w \\leftarrow w - \\eta \\nabla L(x_i)$ | One sample | Fast, randomness helps | Noisy |\n",
    "| Mini-Batch | $w \\leftarrow w - \\eta \\nabla L(B)$ | Batch of data | Balanced, GPU friendly | Needs batch tuning |\n",
    "| SGD + Momentum | $v_t = \\gamma v_{t-1} + \\eta g_t$ | Uses momentum | Faster, smoother | Needs momentum hyperparam |\n",
    "| RMSProp | $w \\leftarrow w - \\frac{\\eta}{\\sqrt{E[g^2]_t}+\\epsilon} g_t$ | Adaptive learning rate | Good for RNNs | May over-adapt |\n",
    "| Adam | Combines Momentum + RMSProp | Adaptive + momentum | Default, fast convergence | Sometimes overfits |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d0d7b3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
