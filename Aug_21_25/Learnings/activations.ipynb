{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86aba9e6",
   "metadata": {},
   "source": [
    "# Activation Functions in Deep Learning  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction  \n",
    "Activation functions introduce **non-linearity** into neural networks.  \n",
    "They decide whether a neuron should be \"activated\" or not, allowing the model to learn complex patterns beyond simple linear relationships.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Key Terms  \n",
    "\n",
    "### Linear Transformation  \n",
    "Before activation, a neuron computes a weighted sum:  \n",
    "\n",
    "$$\n",
    "z = w \\cdot x + b\n",
    "$$  \n",
    "\n",
    "Where:  \n",
    "- $x$: input  \n",
    "- $w$: weight  \n",
    "- $b$: bias  \n",
    "\n",
    "### Activation Function  \n",
    "A mathematical function applied to $z$ to produce the neuron output:  \n",
    "\n",
    "$$\n",
    "a = f(z)\n",
    "$$  \n",
    "\n",
    "- Adds non-linearity.  \n",
    "- Helps the network approximate complex functions.  \n",
    "\n",
    "### Vanishing Gradient  \n",
    "- A problem where gradients become very small during backpropagation.  \n",
    "- Causes weights to update very slowly (common with sigmoid/tanh).  \n",
    "\n",
    "### Exploding Gradient  \n",
    "- Gradients become very large during backpropagation.  \n",
    "- Can cause unstable training.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Types of Activation Functions  \n",
    "\n",
    "### 3.1 Sigmoid Function  \n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$  \n",
    "\n",
    "- Range: $(0, 1)$  \n",
    "- Smooth, S-shaped curve.  \n",
    "- Often used in binary classification (output layer).  \n",
    "\n",
    "**Pros:**  \n",
    "- Maps output to probability-like values.  \n",
    "\n",
    "**Cons:**  \n",
    "- Vanishing gradient problem.  \n",
    "- Not zero-centered.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Hyperbolic Tangent (tanh)  \n",
    "\n",
    "$$\n",
    "f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$  \n",
    "\n",
    "- Range: $(-1, 1)$  \n",
    "- Similar to sigmoid but centered at 0.  \n",
    "\n",
    "**Pros:**  \n",
    "- Better than sigmoid for hidden layers.  \n",
    "\n",
    "**Cons:**  \n",
    "- Still suffers from vanishing gradients.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 ReLU (Rectified Linear Unit)  \n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$  \n",
    "\n",
    "- Range: $[0, \\infty)$  \n",
    "- Very widely used in hidden layers.  \n",
    "\n",
    "**Pros:**  \n",
    "- Computationally efficient.  \n",
    "- Reduces vanishing gradient issue.  \n",
    "\n",
    "**Cons:**  \n",
    "- \"Dying ReLU\" problem (neurons stuck at 0).  \n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Leaky ReLU  \n",
    "\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$  \n",
    "\n",
    "Where $\\alpha$ is a small constant (e.g., 0.01).  \n",
    "\n",
    "**Pros:**  \n",
    "- Solves dying ReLU problem by allowing small negative slope.  \n",
    "\n",
    "**Cons:**  \n",
    "- Adds a small complexity compared to ReLU.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3.5 Parametric ReLU (PReLU)  \n",
    "\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "a x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$  \n",
    "\n",
    "Where $a$ is a learnable parameter.  \n",
    "\n",
    "**Pros:**  \n",
    "- Model learns the negative slope.  \n",
    "\n",
    "**Cons:**  \n",
    "- Slightly more complex than Leaky ReLU.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3.6 Softmax Function  \n",
    "\n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "$$  \n",
    "\n",
    "Where $z_i$ is the score for class $i$ among $K$ classes.  \n",
    "\n",
    "- Converts raw scores into probabilities.  \n",
    "- Used in the **output layer for multi-class classification**.  \n",
    "\n",
    "**Pros:**  \n",
    "- Produces probability distribution (sum = 1).  \n",
    "\n",
    "**Cons:**  \n",
    "- Computationally heavier due to exponential calculations.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3.7 Swish (Google Research)  \n",
    "\n",
    "$$\n",
    "f(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}}\n",
    "$$  \n",
    "\n",
    "- A smooth, non-monotonic function.  \n",
    "- Proposed by Google, often outperforms ReLU.  \n",
    "\n",
    "**Pros:**  \n",
    "- Avoids dying neuron problem.  \n",
    "- Better performance in deeper models.  \n",
    "\n",
    "**Cons:**  \n",
    "- More computationally expensive than ReLU.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Rule of Thumb for Activation Functions  \n",
    "\n",
    "- Hidden layers: ReLU (or Leaky ReLU if dying ReLU is observed).  \n",
    "- Output layer (binary classification): Sigmoid.  \n",
    "- Output layer (multi-class classification): Softmax.  \n",
    "- For experimental deep models: Swish or advanced variants may give better results.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Summary Table  \n",
    "\n",
    "| Function   | Formula | Range | Pros | Cons |\n",
    "|------------|---------|-------|------|------|\n",
    "| Sigmoid    | $\\frac{1}{1+e^{-x}}$ | (0,1) | Probabilistic output | Vanishing gradients |\n",
    "| tanh       | $\\tanh(x)$ | (-1,1) | Zero-centered | Vanishing gradients |\n",
    "| ReLU       | $\\max(0,x)$ | [0,∞) | Efficient, widely used | Dying neurons |\n",
    "| Leaky ReLU | $\\max(\\alpha x, x)$ | (-∞,∞) | Solves dying ReLU | Small added complexity |\n",
    "| PReLU      | Learnable slope | (-∞,∞) | Learns slope automatically | More params |\n",
    "| Softmax    | $\\frac{e^{z_i}}{\\sum_j e^{z_j}}$ | (0,1), sum=1 | Multi-class probs | Expensive computation |\n",
    "| Swish      | $x \\cdot \\sigma(x)$ | (-∞,∞) | Better deep learning results | Slower than ReLU |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd512b3f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
