{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e10a6be",
   "metadata": {},
   "source": [
    "# Training Parameters: Epochs, Batch Size, Iterations, Learning Rate\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Epoch\n",
    "- **Definition:** One epoch means **one complete pass** of the **entire training dataset** through the model.  \n",
    "- Example: If you have 10,000 training samples and you pass all of them once → 1 epoch.  \n",
    "- **Effect:**  \n",
    "  - More epochs → model learns better (up to a limit).  \n",
    "  - Too many epochs → **overfitting**.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Batch Size\n",
    "- **Definition:** Number of samples processed **before updating weights** once.  \n",
    "- If dataset = 10,000 samples and batch size = 100 → each epoch has 100 updates.  \n",
    "- **Effect:**  \n",
    "  - Small batch size → noisy updates, better generalization, slower training.  \n",
    "  - Large batch size → faster on GPU, but risk of poor generalization.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Iteration\n",
    "- **Definition:** One iteration = **one weight update step**.  \n",
    "- Relationship:  \n",
    "  $$\n",
    "  \\text{Iterations per Epoch} = \\frac{\\text{Number of Training Samples}}{\\text{Batch Size}}\n",
    "  $$\n",
    "- Example: Dataset = 10,000 samples, Batch Size = 100 → 100 iterations per epoch.  \n",
    "- If trained for 50 epochs → Total iterations = $50 \\times 100 = 5000$.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Learning Rate (η)\n",
    "- **Definition:** Controls the **step size** at each iteration while moving toward minimum of the loss function.  \n",
    "- Update Rule (basic gradient descent):  \n",
    "  $$\n",
    "  w_{t+1} = w_t - \\eta \\nabla L(w_t)\n",
    "  $$\n",
    "  where:  \n",
    "  - $w_t$: weight at step *t*  \n",
    "  - $\\eta$: learning rate  \n",
    "  - $\\nabla L(w_t)$: gradient of the loss with respect to weight  \n",
    "\n",
    "- **Effect:**  \n",
    "  - Small $\\eta$ → slow learning.  \n",
    "  - Large $\\eta$ → may diverge or oscillate.  \n",
    "  - Optimal $\\eta$ → fast and stable convergence.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Summary Relationships\n",
    "- **Epoch:** 1 full pass of dataset.  \n",
    "- **Batch Size:** number of samples per update.  \n",
    "- **Iteration:** one weight update step.  \n",
    "- **Learning Rate:** size of the update step.  \n",
    "\n",
    "**Formula Recap:**  \n",
    "$$\n",
    "\\text{Iterations per Epoch} = \\frac{\\text{Dataset Size}}{\\text{Batch Size}}\n",
    "$$  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7a0ed",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
